"""
Workspace - Unified face/cluster/track review with internal tabs.

This page provides thumbnail-first review cards with real metrics from
cluster_metrics and track_metrics.
"""

import json
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import streamlit as st

from app.lib.mutator_api import configure_workspace_mutator
from app.lib.pipeline import (
    check_artifacts_status,
    is_pipeline_running,
    check_pipeline_can_run,
    get_maintenance_reason,
    calculate_eta,
    format_eta,
    cancel_pipeline,
)
from app.components.episode_manager_modal import episode_manager_modal
from app.workspace.faces import render_faces_tab
from app.workspace.clusters import render_clusters_tab
from app.workspace.tracks import render_tracks_tab
from app.workspace.review import render_review_tab
from app.utils.ui_keys import safe_rerun
from app.lib.registry import (
    load_registry,
    get_all_episodes,
    recover_episodes_from_fs,
    ensure_episode_in_registry,
    get_default_episode,
    load_episodes_json
)
from app.lib.episode_manager import purge_all_episodes, delete_video_file

# Page config
st.set_page_config(
    page_title="Workspace",
    page_icon="üóÇÔ∏è",
    layout="wide",
)

# Constants
DATA_ROOT = Path("data")


def init_workspace_state():
    """Initialize workspace session state."""
    if "workspace_tab" not in st.session_state:
        st.session_state.workspace_tab = "Clusters"
    if "workspace_episode" not in st.session_state:
        st.session_state.workspace_episode = None
    if "workspace_selected_person" not in st.session_state:
        st.session_state.workspace_selected_person = None
    if "workspace_selected_cluster" not in st.session_state:
        st.session_state.workspace_selected_cluster = None


@st.dialog("‚ö†Ô∏è Purge All Episodes", width="large")
def purge_all_episodes_dialog():
    """Confirmation dialog for purging all episodes."""
    st.warning("**WARNING:** This will permanently archive ALL episodes!")

    st.markdown("""
    **What will happen:**
    - All episodes will be removed from `diagnostics/episodes.json`
    - `data/harvest/<episode>/` will be moved to `data/archive/episodes/<episode>_<timestamp>/`
    - `data/outputs/<episode>/` will be moved to `data/archive/episodes/<episode>_<timestamp>/`
    - Optionally archive video files (you can choose below)
    - **Facebank and show registry will NOT be touched**

    **This cannot be easily undone.**
    """)

    archive_videos = st.checkbox(
        "Also archive video files (recommended to keep videos)",
        value=False,
        key="purge_archive_videos"
    )

    understand = st.checkbox(
        "I understand this will archive ALL episodes",
        key="purge_understand"
    )

    confirm_text = st.text_input(
        'Type "PURGE ALL" to confirm:',
        key="purge_confirm_text"
    )

    reason = st.text_area(
        "Reason (optional)",
        key="purge_reason",
        placeholder="e.g., Starting fresh with new episode uploads"
    )

    col1, col2 = st.columns(2)

    with col1:
        if st.button("Cancel", key="purge_cancel_btn", use_container_width=True):
            st.session_state["_show_purge_dialog"] = False
            st.rerun()

    with col2:
        confirm_valid = understand and confirm_text == "PURGE ALL"

        if st.button(
            "‚úÖ Confirm Purge",
            key="purge_confirm_btn",
            type="primary",
            disabled=not confirm_valid,
            use_container_width=True,
        ):
            try:
                with st.spinner("Purging all episodes..."):
                    stats = purge_all_episodes(
                        archive_videos=archive_videos,
                        actor="user",
                        reason=reason or "Purged all episodes to start fresh"
                    )

                st.success(f"‚úÖ Purged {stats['episodes_purged']} episodes successfully!")

                if stats['errors']:
                    st.warning(f"‚ö†Ô∏è Encountered {len(stats['errors'])} errors:")
                    for err in stats['errors']:
                        st.markdown(f"- {err}")

                # Show summary
                with st.expander("Purge Summary"):
                    st.json(stats)

                st.session_state["_show_purge_dialog"] = False
                st.session_state["_purge_complete"] = True

                # Clear all caches to force registry reload
                st.cache_data.clear()
                st.cache_resource.clear()

                # Clear episode selection
                st.session_state.pop("workspace_episode", None)
                st.session_state.pop("episode_id", None)

                st.rerun()

            except Exception as e:
                st.error(f"‚ùå Purge failed: {str(e)}")


def main():
    """Main workspace page."""
    init_workspace_state()

    # Inject 4:5 thumbnail CSS - FILL frame with cover (no letterboxing)
    st.markdown("""
    <style>
    /* Force all thumbnails to FILL 4:5 frame */
    .tile-45 {
        width: 160px;
        height: 200px;
        overflow: hidden;
        border-radius: 8px;
        background: #f6f6f6;
        display: inline-block;
    }
    .tile-45 img {
        width: 100%;
        height: 100%;
        object-fit: cover !important;
        object-position: center !important;
        display: block;
    }
    /* Apply to all st.image containers */
    div[data-testid="stImage"] {
        width: 160px !important;
        height: 200px !important;
        overflow: hidden !important;
    }
    div[data-testid="stImage"] img {
        width: 100% !important;
        height: 100% !important;
        object-fit: cover !important;
        object-position: center !important;
        display: block !important;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title("üóÇÔ∏è Workspace")

    # Load registry using canonical loader
    reg = load_registry()
    eps_in_registry = get_all_episodes(reg)
    eps_on_disk = recover_episodes_from_fs()

    # Recovery UI if registry is empty but episodes exist on disk
    if not eps_in_registry and eps_on_disk:
        st.warning("‚ö†Ô∏è No episodes in registry. Found harvests on disk.")
        st.info("üìÅ Select an episode below to add it to the registry and continue.")

        selected = st.selectbox(
            "Recover episode",
            options=eps_on_disk,
            key="recover_ep_select"
        )

        col1, col2 = st.columns(2)
        with col1:
            show_id = st.text_input("Show ID", "rhobh", key="recover_show_id")
        with col2:
            season_id = st.text_input("Season ID", "s05", key="recover_season_id")

        if st.button("Add to registry & continue", key="recover_ep_btn", type="primary") and selected:
            ensure_episode_in_registry(selected, show_id, season_id)
            st.stop()

        st.stop()

    elif not eps_in_registry and not eps_on_disk:
        st.error("‚ùå No episodes found in registry or data/harvest.")
        st.info("Please upload an episode first using the Upload page.")
        st.stop()

    # Episode selector (normal path)
    # Extract just episode_ids from tuples
    episode_ids = [ep_id for _, _, ep_id in eps_in_registry]

    if not episode_ids:
        st.warning("No episodes found in registry.")

        # Show recovery option if harvest episodes exist
        if eps_on_disk:
            st.info("Found episodes in data/harvest. Use recovery option above.")
        return

    # Surface thumbnail generation summary if present
    thumb_summary = st.session_state.pop("_thumb_summary", None)
    if thumb_summary:
        if thumb_summary.get("error_message"):
            st.error(thumb_summary["error_message"])
        else:
            st.success(
                f"Generated {thumb_summary['generated']} of {thumb_summary['total_tracks']} thumbnails"
                f" (placeholders: {thumb_summary['placeholders']})."
            )
            if thumb_summary.get("errors"):
                st.info(
                    f"{thumb_summary['errors']} tracks could not be thumbnailed."
                    " See thumbnails_stats.json for details."
                )

    # Header row: Pipeline buttons + Episode selector
    header_cols = st.columns([1.2, 1, 1, 2.5, 0.5])

    # Import pipeline helpers

    # Get default episode using smart selection
    default_ep = get_default_episode(episode_ids)
    current_ep = st.session_state.get("episode_id", default_ep)
    pipeline_running = is_pipeline_running(current_ep, DATA_ROOT) if current_ep else False
    artifact_status = check_artifacts_status(current_ep, DATA_ROOT) if current_ep else {
        "prepared": False,
        "has_clusters": False
    }


    # Check if pipeline can run (maintenance mode gate)
    pipeline_check = check_pipeline_can_run(current_ep, DATA_ROOT) if current_ep else {"can_run": True, "reason": ""}
    can_run = pipeline_check.get("can_run", True)
    block_reason = pipeline_check.get("reason", "")

    # Phase 3 P1: Check extraction status from episode registry
    extraction_ready = False
    episode_state = None
    if current_ep:
        from api.episodes import get_episode_state
        episode_state = get_episode_state(current_ep)
        extraction_ready = episode_state.get("states", {}).get("extracted_frames", False)

        # Auto-refresh while extraction is in progress
        if not extraction_ready:
            # Check if extraction is actually running or just pending
            validated = episode_state.get("states", {}).get("validated", False)

            if validated:
                # Validated but not extracted yet - poll for completion
                import time

                # Add auto-refresh banner
                st.info("‚è≥ Frame extraction in progress... Page will refresh automatically.")

                # Wait a bit then refresh (simulates polling)
                poll_interval = 2  # seconds
                if "last_poll_time" not in st.session_state:
                    st.session_state["last_poll_time"] = time.time()

                elapsed = time.time() - st.session_state["last_poll_time"]

                if elapsed >= poll_interval:
                    st.session_state["last_poll_time"] = time.time()
                    st.rerun()

                # Show progress placeholder
                with st.empty():
                    st.caption(f"Checking again in {max(0, poll_interval - int(elapsed))} seconds...")
                    time.sleep(0.5)
                    st.rerun()

    with header_cols[0]:
        # Phase 3 P1: Conditional Prepare based on extraction status
        if extraction_ready:
            # Frames ready - show full pipeline button
            prepare_help = "Run full pipeline: RetinaFace (detect) ‚Üí ArcFace (embed) ‚Üí ByteTrack (track) ‚Üí Agglomerative Clustering"
            if not can_run:
                prepare_help = f"Blocked: {block_reason}"

            if st.button(
                "(1‚Äì4) Run Full Pipeline",
                type="primary",
                help=prepare_help,
                key="workspace_prepare_btn",
                use_container_width=True,
                disabled=not can_run,
            ):
                st.session_state["_trigger_prepare"] = True
        else:
            # Frames not extracted yet - show passive message
            st.info("‚è≥ Extracting frames...")
            st.caption("This happens automatically after upload validation")

            # Add manual trigger in case auto-extraction failed
            if st.button(
                "‚ö†Ô∏è Retry Extraction",
                type="primary",
                use_container_width=True,
                help="Manual frame extraction if auto-extraction failed"
            ):
                from jobs.tasks.auto_extract import trigger_auto_extraction
                with st.spinner("Extracting frames..."):
                    try:
                        from pathlib import Path
                        video_path = episode_state.get("video_path", "")
                        if not video_path or not Path(video_path).exists():
                            st.error(f"‚ùå Video file not found: {video_path}")
                        else:
                            result = trigger_auto_extraction(current_ep, video_path)
                            if result.get("success"):
                                st.success("‚úÖ Extraction complete!")
                                st.rerun()
                            else:
                                st.error(f"‚ùå Extraction failed: {result.get('error')}")
                    except Exception as e:
                        st.error(f"‚ùå Extraction error: {str(e)}")


    with header_cols[1]:
        cluster_disabled = not can_run or not artifact_status["prepared"]
        cluster_help = "Cluster prepared tracks against current facebank"
        if not can_run:
            cluster_help = f"Blocked: {block_reason}"
        elif not artifact_status["prepared"]:
            cluster_help = "Requires preparation first"
        
        if st.button(
            "4. Clustering (Group Tracks)",
            help=cluster_help,
            key="workspace_cluster_btn",
            use_container_width=True,
            disabled=cluster_disabled,
        ):
            st.session_state["_trigger_cluster"] = True

    with header_cols[2]:
        analyze_disabled = not can_run or not artifact_status["has_clusters"]
        analyze_help = "Generate timeline & totals from clusters"
        if not can_run:
            analyze_help = f"Blocked: {block_reason}"
        elif not artifact_status["has_clusters"]:
            analyze_help = "Requires clustering first"
        
        if st.button(
            "üìä Analyze",
            help=analyze_help,
            key="workspace_analyze_btn",
            use_container_width=True,
            disabled=analyze_disabled,
        ):
            st.session_state["_trigger_analyze"] = True


    with header_cols[3]:
        # Use get_default_episode for smart selection
        default_idx = 0
        if default_ep and default_ep in episode_ids:
            default_idx = episode_ids.index(default_ep)
        elif st.session_state.workspace_episode in episode_ids:
            default_idx = episode_ids.index(st.session_state.workspace_episode)

        selected_episode = st.selectbox(
            "Episode",
            options=episode_ids,
            index=default_idx,
            key="workspace_episode_selector",
            label_visibility="collapsed"
        )

    with header_cols[4]:
        if st.button("‚Üª", key="refresh_episodes_btn", help="Refresh episode list", use_container_width=True):
            # Clear episode cache and reload
            st.session_state.pop("episode_id", None)
            st.cache_data.clear()
            safe_rerun()


    # Manage Episode button - opens modal
    col_manage, col_purge = st.columns([1, 1])

    with col_manage:
        if current_ep:
            if st.button("üîß Manage Episode", key="manage_episode_btn", use_container_width=False):
                episode_manager_modal(current_ep, DATA_ROOT)

    with col_purge:
        if st.button("üóëÔ∏è Remove ALL Episodes", key="purge_all_btn", use_container_width=False, type="secondary"):
            st.session_state["_show_purge_dialog"] = True

    # Show purge dialog if triggered
    if st.session_state.get("_show_purge_dialog"):
        purge_all_episodes_dialog()

    # Show success message if purge just completed
    if st.session_state.pop("_purge_complete", False):
        st.success("‚úÖ All episodes have been purged. Episode dropdowns will be empty until new episodes are uploaded.")
        safe_rerun()

    # Update session state if episode changed
    if selected_episode != st.session_state.workspace_episode:
        st.session_state.workspace_episode = selected_episode
        st.session_state.episode_id = selected_episode  # For wkey()
        st.session_state.workspace_selected_person = None
        st.session_state.workspace_selected_cluster = None
        safe_rerun()

    # Store episode_id for wkey
    st.session_state.episode_id = selected_episode

    # Get show_id and season_id for the selected episode
    show_id, season_id = None, None
    for s_id, ss_id, ep_id in eps_in_registry:
        if ep_id == selected_episode:
            show_id, season_id = s_id, ss_id
            break

    # Progress polling UI
    from app.workspace.common import read_pipeline_state
    pipeline_state = read_pipeline_state(selected_episode, DATA_ROOT)

    # Handle cancelled state (show toast once)
    if pipeline_state and pipeline_state.get("status") == "cancelled":
        cancelled_key = f"_pipeline_cancelled_{selected_episode}"
        if cancelled_key not in st.session_state:
            st.info("‚úÖ Pipeline was cancelled")
            st.session_state[cancelled_key] = True

            # Archive the cancelled state to prevent re-showing
            state_file = DATA_ROOT / "harvest" / selected_episode / "diagnostics" / "pipeline_state.json"
            if state_file.exists():
                try:
                    with open(state_file, "r") as f:
                        final_state = json.load(f)
                    final_state["status"] = "archived"
                    with open(state_file, "w") as f:
                        json.dump(final_state, f, indent=2)
                except Exception:
                    pass

    # Handle done state (show toast once)
    if pipeline_state and pipeline_state.get("status") == "done":
        done_key = f"_pipeline_done_{selected_episode}"
        if done_key not in st.session_state:
            # Show success toast with summary
            extra = pipeline_state.get("extra", {})
            result = extra.get("result", {}) if extra else {}

            summary_parts = []
            if "total_detections" in result:
                summary_parts.append(f"{result['total_detections']} faces detected")
            if "n_tracks" in result:
                summary_parts.append(f"{result['n_tracks']} tracks")
            if "n_clusters" in result:
                summary_parts.append(f"{result['n_clusters']} clusters")
            if "generated" in result:
                summary_parts.append(f"{result['generated']} stills")

            # Check for analytics totals
            totals_by_identity = result.get("totals_by_identity", {})
            if totals_by_identity:
                identity_count = len([k for k in totals_by_identity.keys() if k != "Unknown"])
                if identity_count > 0:
                    summary_parts.append(f"{identity_count} identities tracked")

            summary_msg = ", ".join(summary_parts) if summary_parts else "completed"
            st.toast(f"‚úÖ Pipeline complete: {summary_msg}", icon="‚úÖ")
            st.session_state[done_key] = True

            # Archive the state file to prevent re-showing
            try:
                from screentime.diagnostics.utils import archive_pipeline_state
                archive_pipeline_state(selected_episode)
            except Exception:
                # Fallback: just mark as archived
                state_file = DATA_ROOT / "harvest" / selected_episode / "diagnostics" / "pipeline_state.json"
                if state_file.exists():
                    with open(state_file, "r") as f:
                        final_state = json.load(f)
                    final_state["status"] = "archived"
                    with open(state_file, "w") as f:
                        json.dump(final_state, f, indent=2)

    if pipeline_state and pipeline_state.get("status") not in (None, "done", "archived", "cancelled"):
        st.markdown("---")

        current_step = pipeline_state.get("current_step", "Unknown")
        step_index = pipeline_state.get("step_index", 0)
        total_steps = pipeline_state.get("total_steps", 4)
        status = pipeline_state.get("status", "unknown")
        message = pipeline_state.get("message", "")

        if status == "error":
            st.error(f"‚ùå Pipeline error: {message}")

            with st.expander("Error details"):
                st.json(pipeline_state)

            # Check if error is in Stills stage
            is_stills_error = "stills" in current_step.lower() or "generate" in current_step.lower()

            if is_stills_error:
                # Stills-specific retry options
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("üîÅ Resume Stills", key="resume_stills_btn", help="Continue from where it stalled"):
                        with st.spinner("Resuming stills generation..."):
                            try:
                                from jobs.tasks.orchestrate import run_stills_only
                                result = run_stills_only(selected_episode, data_root=DATA_ROOT, resume=True, force=False)
                                if result.get("status") == "ok":
                                    st.success("Stills generation resumed successfully!")
                                    safe_rerun()
                                else:
                                    st.error(f"Resume failed: {result.get('error', 'Unknown error')}")
                            except Exception as exc:
                                st.error(f"Resume failed: {exc}")
                with col2:
                    if st.button("üîÑ Force Re-run Stills", key="force_stills_btn", help="Regenerate all stills from scratch"):
                        with st.spinner("Regenerating all stills..."):
                            try:
                                from jobs.tasks.orchestrate import run_stills_only
                                result = run_stills_only(selected_episode, data_root=DATA_ROOT, resume=False, force=True)
                                if result.get("status") == "ok":
                                    st.success("Stills regenerated successfully!")
                                    safe_rerun()
                                else:
                                    st.error(f"Regeneration failed: {result.get('error', 'Unknown error')}")
                            except Exception as exc:
                                st.error(f"Regeneration failed: {exc}")
                with col3:
                    if st.button("‚ùå Clear Error", key="clear_error_btn"):
                        state_file = DATA_ROOT / "harvest" / selected_episode / "diagnostics" / "pipeline_state.json"
                        if state_file.exists():
                            state_file.unlink()
                        safe_rerun()
            else:
                # General retry options
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("üîÑ Retry Pipeline", key="retry_pipeline_btn"):
                        st.session_state["_trigger_cluster"] = True
                        safe_rerun()
                with col2:
                    if st.button("‚ùå Clear Error", key="clear_error_btn_gen"):
                        state_file = DATA_ROOT / "harvest" / selected_episode / "diagnostics" / "pipeline_state.json"
                        if state_file.exists():
                            state_file.unlink()
                        safe_rerun()
        else:
            # Running state
            overall_pct = (step_index / total_steps) if total_steps > 0 else 0.0

            col_status, col_cancel = st.columns([4, 1])
            with col_status:
                st.info(f"üîÑ {current_step} ({step_index}/{total_steps})")
            with col_cancel:
                if st.button("‚èπÔ∏è Cancel Pipeline", key="cancel_pipeline_btn", type="secondary", use_container_width=True):
                    if cancel_pipeline(selected_episode, DATA_ROOT):
                        st.success("‚úÖ Pipeline cancelled")
                        safe_rerun()
                    else:
                        st.error("‚ùå Failed to cancel pipeline")

            if message:
                st.caption(message)

            # Per-stage progress bars
            stages = ["Detect/Embed", "Track", "Cluster", "Generate Stills", "Analytics"]
            # Map display names to step keys used in stats
            step_keys = ["detect", "track", "cluster", "stills", "analytics"]

            for i, stage_name in enumerate(stages, start=1):
                if i < step_index:
                    # Completed
                    st.progress(1.0, text=f"‚úÖ {stage_name}")
                elif i == step_index:
                    # Current - add ETA if available
                    stage_pct = pipeline_state.get("pct", 0.5)
                    if stage_pct is None:
                        stage_pct = 0.5

                    # Calculate ETA for current step
                    try:
                        step_key = step_keys[i - 1]
                        operation = "prepare"  # Default operation

                        # Determine operation type from pipeline state
                        if "cluster" in current_step.lower():
                            operation = "cluster"
                        elif "analyt" in current_step.lower():
                            operation = "analytics"

                        # Get remaining steps
                        remaining_steps = [step_keys[j] for j in range(i, len(step_keys))]

                        eta_info = calculate_eta(
                            selected_episode,
                            operation,
                            step_key,
                            remaining_steps,
                            current_step_elapsed_s=0.0,
                            data_root=DATA_ROOT
                        )

                        eta_seconds = eta_info.get("eta_seconds", 0)
                        confidence = eta_info.get("confidence", "none")

                        if confidence == "none" or eta_seconds <= 0:
                            eta_text = " ‚Ä¢ learning ETA..."
                        else:
                            eta_formatted = format_eta(eta_seconds)
                            eta_text = f" ‚Ä¢ ETA: ~{eta_formatted}"
                    except Exception:
                        eta_text = ""

                    st.progress(float(stage_pct), text=f"‚è≥ {stage_name}{eta_text}")
                else:
                    # Pending
                    st.progress(0.0, text=f"‚è∏Ô∏è {stage_name}")

            # Auto-refresh every 2 seconds
            if "last_refresh_ts" not in st.session_state:
                st.session_state.last_refresh_ts = 0

            import time
            current_ts = time.time()
            if current_ts - st.session_state.last_refresh_ts > 2:
                st.session_state.last_refresh_ts = current_ts
                st.rerun()

        st.markdown("---")

    # Handle Prepare button click
    if st.session_state.pop("_trigger_prepare", False):
        st.info("üîÑ **Starting Prepare pipeline...**")
        st.write("Running Detect/Embed ‚Üí Track ‚Üí Generate Face Stills")

        # Mark pipeline as starting
        from app.workspace.common import read_pipeline_state
        diagnostics_dir = DATA_ROOT / "harvest" / selected_episode / "diagnostics"
        diagnostics_dir.mkdir(parents=True, exist_ok=True)

        state_file = diagnostics_dir / "pipeline_state.json"
        with open(state_file, "w") as f:
            json.dump({
                "episode": selected_episode,
                "current_step": "Starting",
                "step_index": 0,
                "total_steps": 4,
                "status": "running",
                "message": "Initializing Prepare pipeline...",
            }, f, indent=2)

        try:
            from jobs.tasks.orchestrate import orchestrate_prepare

            with st.spinner("Running Prepare pipeline... This may take several minutes."):
                result = orchestrate_prepare(
                    episode_id=selected_episode,
                    data_root=DATA_ROOT,
                    force=False,  # Skip stages that already have artifacts
                    resume=True,  # Resume from last stage
                )

            if result.get("status") == "ok":
                st.success(f"‚úÖ Prepare complete for {selected_episode}!")
                st.info("üí° **Next steps:**\n1. Curate facebank on CAST page\n2. Click **Cluster** button")
                safe_rerun()
            else:
                st.error(f"‚ùå Prepare failed: {result.get('error', 'Unknown error')}")
                with st.expander("Error details"):
                    st.json(result)

        except Exception as e:
            st.error(f"‚ùå Prepare failed: {str(e)}")
            import traceback
            with st.expander("Error details"):
                st.code(traceback.format_exc())

    # Handle Cluster button click
    if st.session_state.pop("_trigger_cluster", False):
        st.info("üéØ **Starting Cluster pipeline...**")
        st.write("Clustering prepared tracks against current facebank")

        # Mark pipeline as starting
        diagnostics_dir = DATA_ROOT / "harvest" / selected_episode / "diagnostics"
        diagnostics_dir.mkdir(parents=True, exist_ok=True)
        state_file = diagnostics_dir / "pipeline_state.json"

        with open(state_file, "w") as f:
            json.dump({
                "episode": selected_episode,
                "current_step": "Cluster",
                "step_index": 1,
                "total_steps": 1,
                "status": "running",
                "message": "Clustering tracks...",
            }, f, indent=2)

        try:
            from jobs.tasks.orchestrate import orchestrate_cluster_only

            with st.spinner("Clustering... This may take a few minutes."):
                result = orchestrate_cluster_only(
                    episode_id=selected_episode,
                    data_root=DATA_ROOT,
                )

            if result.get("status") == "ok":
                n_clusters = result.get("result", {}).get("n_clusters", 0)
                st.success(f"‚úÖ Clustered into {n_clusters} clusters!")
                st.info("üí° **Next step:** Click **Analyze** to generate timeline & totals")
                safe_rerun()
            else:
                st.error(f"‚ùå Cluster failed: {result.get('error', 'Unknown error')}")
                with st.expander("Error details"):
                    st.json(result)

        except Exception as e:
            st.error(f"‚ùå Cluster failed: {str(e)}")
            import traceback
            with st.expander("Error details"):
                st.code(traceback.format_exc())

    # Handle Analyze button click
    if st.session_state.pop("_trigger_analyze", False):
        st.info("üìä **Starting Analytics pipeline...**")
        st.write("Generating timeline & totals from final cluster labels")

        # Mark pipeline as starting
        diagnostics_dir = DATA_ROOT / "harvest" / selected_episode / "diagnostics"
        diagnostics_dir.mkdir(parents=True, exist_ok=True)
        state_file = diagnostics_dir / "pipeline_state.json"

        with open(state_file, "w") as f:
            json.dump({
                "episode": selected_episode,
                "current_step": "Analytics",
                "step_index": 1,
                "total_steps": 1,
                "status": "running",
                "message": "Generating analytics...",
            }, f, indent=2)

        try:
            from jobs.tasks.orchestrate import orchestrate_analytics_only

            with st.spinner("Generating analytics..."):
                result = orchestrate_analytics_only(
                    episode_id=selected_episode,
                    data_root=DATA_ROOT,
                )

            if result.get("status") == "ok":
                intervals = result.get("result", {}).get("intervals_created", 0)
                st.success(f"‚úÖ Analytics complete! Generated {intervals} timeline intervals")
                st.info("üí° Check the **Review** tab to see timeline & totals")
                safe_rerun()
            else:
                st.error(f"‚ùå Analytics failed: {result.get('error', 'Unknown error')}")
                with st.expander("Error details"):
                    st.json(result)

        except Exception as e:
            st.error(f"‚ùå Analytics failed: {str(e)}")
            import traceback
            with st.expander("Error details"):
                st.code(traceback.format_exc())

    # Keep old cluster handler as fallback (for Enhance button compatibility)
    if st.session_state.pop("_trigger_cluster_full", False):
        st.info("üîÑ Starting full pipeline: Detect/Embed ‚Üí Track ‚Üí Cluster ‚Üí Generate Stills...")

    # Handle Enhance Clusters button click
    if st.session_state.pop("_trigger_enhance", False):
        st.info("‚ú® Starting enhance clusters: re-cluster with constraints + densify + analytics...")

        try:
            from jobs.tasks.recluster import recluster_task
            from jobs.tasks.densify_two_pass import densify_two_pass
            from jobs.tasks.analytics import analytics_task
            from app.lib.data import load_clusters
            import yaml

            # Load config to check if densify is enabled
            config_path = Path("configs/pipeline.yaml")
            config = {}
            if config_path.exists():
                with open(config_path) as f:
                    config = yaml.safe_load(f) or {}

            clustering_config = config.get('clustering', {})
            use_densify = clustering_config.get('use_densify_two_pass', False)

            with st.spinner("Step 1: Re-clustering with manual constraints..."):
                recluster_result = recluster_task(
                    "manual",
                    selected_episode,
                    show_id=show_id,
                    season_id=season_id,
                    sources=None,
                    use_constraints=True  # Always use constraints for enhance
                )
                st.toast(f"‚úÖ Re-clustered into {recluster_result.get('n_clusters', 0)} clusters")

            # Optional densify step
            if use_densify:
                with st.spinner("Step 2: Densifying clusters (two-pass)..."):
                    densify_result = densify_two_pass("manual", selected_episode)
                    st.toast(f"‚úÖ Densified {densify_result.get('tracks_promoted', 0)} tracks")

            with st.spinner(f"Step {3 if use_densify else 2}: Regenerating analytics..."):
                clusters_data = load_clusters(selected_episode, DATA_ROOT)
                cluster_assignments = {}
                if clusters_data:
                    for cluster in clusters_data.get("clusters", []):
                        if "name" in cluster:
                            cluster_assignments[cluster["cluster_id"]] = cluster["name"]

                analytics_result = analytics_task("manual", selected_episode, cluster_assignments)
                st.toast(f"‚úÖ Generated {analytics_result['stats']['intervals_created']} intervals")

            st.success(f"‚úÖ Enhance clusters complete for {selected_episode}!")
            st.info("üí° Analytics are now fresh. Metrics have been recomputed with current constraints.")
            safe_rerun()

        except Exception as e:
            st.error(f"‚ùå Enhance clusters failed: {str(e)}")
            import traceback
            with st.expander("Error details"):
                st.code(traceback.format_exc())

    # Configure mutator
    mutator = configure_workspace_mutator(selected_episode, DATA_ROOT)

    if mutator is None:
        st.error(f"No cluster data found for {selected_episode}. Run clustering first.")
        return

    st.markdown("---")

    # Render tabs internally using radio
    tab = st.radio(
        "Workspace",
        ["Faces", "Clusters", "Tracks", "Review"],
        horizontal=True,
        label_visibility="collapsed",
        index=["Faces", "Clusters", "Tracks", "Review"].index(
            st.session_state.workspace_tab
        ),
        key="workspace_tab_radio",
    )

    # Update session state if tab changed
    if tab != st.session_state.workspace_tab:
        st.session_state.workspace_tab = tab

    # Render selected tab
    if tab == "Faces":
        render_faces_tab(mutator)
    elif tab == "Clusters":
        render_clusters_tab(mutator)
    elif tab == "Tracks":
        render_tracks_tab(mutator)
    else:  # Review
        render_review_tab(mutator)


if __name__ == "__main__":
    main()
